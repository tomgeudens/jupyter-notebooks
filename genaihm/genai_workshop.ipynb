{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "I8F3XGz_dyXc",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Neo4j Generative AI Workshop\n",
    "\n",
    "In this workshop, you will learn how to use Neo4j Knowledge Graphs to make Large Language Models (LLMs) useful for more real-world use cases.\n",
    "\n",
    "We walk through an example that uses real-world customer and product data from a fashion, style, and beauty retailer. We show how you can use a knowledge graph to ground an LLM, enabling it to build tailored marketing content personalized to each customer based on their interests and shared purchase histories. We use a pattern called Retrieval-Augmented Generation (RAG) to accomplish this.  Specifically, one that leverages not only vector search but also graph pattern matching and graph machine learning to provide more relevant personalized results to customers.\n",
    "\n",
    "This notebook walks through the end-to-end process, including:\n",
    "- Building the knowledge graph\n",
    "- Vector search & text embedding\n",
    "- Using graph patterns in Cypher to improve semantic search with context\n",
    "- Further augmenting semantic search with knowledge graph inference & ML\n",
    "- Building the LLM chain and demo app for generating content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cmjr1dz8dyXd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the necessary package installs\n",
    "# not needed in the Summit environment but you will need them if you run this yourself\n",
    "\n",
    "# %pip install sentence_transformers langchain langchain-openai openai tiktoken python-dotenv gradio graphdatascience altair neo4j_tools\n",
    "# %pip install \"vegafusion[embed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7psF1otOdyXe"
   },
   "outputs": [],
   "source": [
    "# the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j_tools import gds_db_load, gds_utils\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import gradio as gr\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def pretty_print(df, num_rows=5):\n",
    "    return display(HTML(df.head(num_rows).to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-98NuINdyXe"
   },
   "outputs": [],
   "source": [
    "# note that in the Summit environment we provide\n",
    "# - a Neo4j database that includes the Graph Data Science library\n",
    "# - an OpenAI key (which will be expired right after the session ;-)\n",
    "# you will need to provide your own if you want to try this later\n",
    "\n",
    "# setting the environment\n",
    "if os.path.exists('ws.env'):\n",
    "    load_dotenv('ws.env', override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    HOST = os.getenv('HOST')\n",
    "    USERNAME = os.getenv('USERNAME')\n",
    "    PASSWORD = os.getenv('PASSWORD')\n",
    "    DATABASE = os.getenv('DATABASE')\n",
    "\n",
    "    # AI\n",
    "    LLM = os.getenv('LLM')\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    os.environ['OPENAI_API_KEY']=OPENAI_API_KEY\n",
    "    \n",
    "print(\"Environment loaded\")\n",
    "\n",
    "# setting the environment manually\n",
    "\n",
    "# Neo4j\n",
    "# HOST = input(\"Enter the connection string (default 'neo4j://localhost:7686'):\") or \"neo4j://localhost:7686\"\n",
    "# USERNAME = input(\"Enter the username (default 'neo4j'):\") or \"neo4j\"\n",
    "# PASSWORD = getpass(\"Enter the password:\") or \"notthepassword\"\n",
    "# DATABASE = input(\"Enter the database (default 'neo4j'):\") or \"neo4j\"\n",
    "\n",
    "# AI\n",
    "# LLM = 'gpt-4'\n",
    "\n",
    "# OpenAI - Required when using OpenAI models\n",
    "# os.environ['OPENAI_API_KEY'] = 'your key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "67Tm1p3LdyXe",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge Graph Building\n",
    "\n",
    "<img src=\"https://github.com/neo4j-product-examples/genai-workshop/blob/main/img/hm-banner.png?raw=true\" alt=\"HM Banner\" width=\"2000\"/>\n",
    "\n",
    "We begin by building our knowledge graph. This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "Below is the graph data model we will use:\n",
    "\n",
    "<img src=\"https://metis.graphdatabase.ninja/images/GraphSummit2024GenAIModelHM.png\" alt=\"Data Model\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yU13AU73dyXf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Connect to Neo4j\n",
    "\n",
    "We will use the [Graph Data Science Python Client](https://neo4j.com/docs/graph-data-science-client/current/) to connect to Neo4j. This client makes it convenient to display results, as we will see later.  Perhaps more importantly, it allows us to easily run [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/introduction/) algorithms from Python.\n",
    "\n",
    "This client will only work if your Neo4j instance has Graph Data Science installed.  If not, you can still use the [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/) or use Langchainâ€™s Neo4j Graph object that we will see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92GFeMaRdyXf"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# GDS connection\n",
    "gds = GraphDataScience(HOST, auth=(USERNAME, PASSWORD), database=DATABASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mO32Q0Thw5JO",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Test your connection by running the below.  It should output your system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJ6qg0qMw5JO"
   },
   "outputs": [],
   "source": [
    "gds.debug.sysInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2H3Yb5JLdyXf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exploring the database\n",
    "The dataset is preloaded for you (and you will get the scripts and files to do so yourself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total node counts\n",
    "gds.run_cypher('''\n",
    "    CALL apoc.meta.stats()\n",
    "    YIELD labels\n",
    "    UNWIND keys(labels) AS nodeLabel\n",
    "    RETURN nodeLabel, labels[nodeLabel] AS nodeCount\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total relationship counts\n",
    "gds.run_cypher('''\n",
    "    CALL apoc.meta.stats()\n",
    "    YIELD relTypesCount\n",
    "    UNWIND keys(relTypesCount) AS relationshipType\n",
    "    RETURN relationshipType, relTypesCount[relationshipType] AS relationshipCount\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move to the Neo4j Browser\n",
    "Let's have a look at the data in the <a href=\"https://browser.neo4j.io/index.html\" target=\"_blank\">Neo4j Browser</a>!\n",
    "\n",
    "Click the above link, a new tab in your web browser should open. Username and password are identical to those you used to get on this environment. Once logged in you should find yourself in your own database. Next you'll be asked to run a browserguide. For that you'll need to copy-paste this (your trainer will explain where):\n",
    "\n",
    "`:play https://metis.graphdatabase.ninja/bredags/exploration.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SpKcGgRZdyXg",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Vector Search\n",
    "\n",
    "In this section, we will build text embeddings out of product descriptions and demonstrate how to leverage the Neo4j vector index for vector search. We will also introduce the use of [LangChain](https://www.langchain.com/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-WrlFCN1dyXg",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Creating Text Embeddings\n",
    "\n",
    "To start we need to make embeddings for our product nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zuqGNOqTZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, we will instantiate our embedding model. This notebook has just been tested with OpenAI, but these models are pluggable. You could choose embedding models from other providers, including cloud providers like AWS Bedrock and Vertex AI Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwRdCBawdyXg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embedding_dimension = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FzzEFpJPZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's create a dataframe with a text column to embed.  In this case, we will combine multiple text columns, such as product name, type, description, etc.  This provides the embedding model with more context.  Some products are missing a description (a small minority).  For our intents and purposes we will leave them out. In a more in-depth workflow, you would likely want to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get product information from the database into a dataframe\n",
    "product_emb_df = gds.run_cypher('''\n",
    "    MATCH (p:Product)\n",
    "    WHERE p.description IS NOT NULL\n",
    "    RETURN p.id as id, p.name as productname, p.type as producttype, p.group as group, p.garment as garment, p.description as description\n",
    "''')\n",
    "\n",
    "# transformation function \n",
    "def create_doc(row):\n",
    "    return f'''##Product\n",
    "Name: {row.productname}\n",
    "Type: {row.producttype}\n",
    "Group: {row.group}\n",
    "Garment Type: {row.garment}\n",
    "Description: {row.description}\n",
    "'''\n",
    "\n",
    "# apply the transformation\n",
    "product_emb_df['text'] = product_emb_df.apply(create_doc, axis = 1)\n",
    "\n",
    "# remove the information we no longer need\n",
    "product_emb_df = product_emb_df.drop(columns=['productname', 'producttype', 'group', 'garment', 'description'])\n",
    "\n",
    "# show the result\n",
    "pretty_print(product_emb_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bLpMaWmkZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now letâ€™s embed the text with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qUudN0KdyXh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = []\n",
    "count = 0\n",
    "\n",
    "# if we would do one by one we'd make 8044 calls to OpenAI at this point\n",
    "# so we chunk the input (which in this case is combining rather than splitting)\n",
    "for chunk in gds_db_load.chunks(product_emb_df.text,500):\n",
    "    embeddings.extend(embedding_model.embed_documents(chunk))\n",
    "    count += len(chunk)\n",
    "    print(f'Embedded {count} of {product_emb_df.shape[0]}')\n",
    "\n",
    "# merge the generated embeddings into the dataframe\n",
    "product_emb_df['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the (first five rows of the) result\n",
    "pretty_print(product_emb_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zOudn8tmdyXo",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Create Vector Properties and Index\n",
    "\n",
    "Now, we will load the embeddings into Neo4j by MATCHing on id, then calling the `db.create.setNodeVectorProperty` to set the embedding property. This special function is used to set the properties as floats rather than double precision, which requires more space.  This becomes important as these embedding vectors tend to be long, and the size can add up quickly.\n",
    "\n",
    "After bulk loading, we will create the vector index. The [Neo4j Vector Index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) enables efficient Approximate Nearest Neighbor (ANN) search with vectors. It uses the Hierarchical Navigable Small World (HNSW) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWtG_wWKdyXo"
   },
   "outputs": [],
   "source": [
    "# load vector properties\n",
    "records = product_emb_df[['id', 'text', 'embedding']].to_dict('records') # creates a list of objects\n",
    "print(f'======  loading Product text embeddings ======')\n",
    "total = len(records)\n",
    "print(f'staging {total:,} records')\n",
    "cumulative_count = 0\n",
    "\n",
    "# we use the chunks function again to create batches of results\n",
    "# that we can process in one transaction (rather than doing them one by one)\n",
    "for recs in gds_db_load.chunks(records, 400):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $recs AS rec\n",
    "    MATCH(n:Product {id: rec.id})\n",
    "    SET n.text = rec.text\n",
    "    WITH n, rec\n",
    "    CALL db.create.setNodeVectorProperty(n, \"embedding\", rec.embedding)\n",
    "    RETURN count(n) AS propertySetCount\n",
    "    ''', params={'recs': recs})\n",
    "    cumulative_count += res.iloc[0, 0]\n",
    "    print(f'Set {cumulative_count:,} of {total:,} text embeddings')\n",
    "\n",
    "# create index\n",
    "gds.run_cypher('''\n",
    "CREATE VECTOR INDEX product_embedding IF NOT EXISTS FOR (n:Product) ON (n.embedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dim),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dim': embedding_dimension})\n",
    "\n",
    "gds.run_cypher('CALL db.awaitIndex(\"product_embedding\", 300)')\n",
    "print(\"Vector index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rGoMu3bqekeE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Vector Search Using Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SN7zU0GbZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To do vector search, we need to:\n",
    "1. Take the search prompt and convert it to an embedding query vector\n",
    "2. Use similarity search with that new vector to pull semantically similar documents\n",
    "\n",
    "Below is an example of converting a search prompt into a query vector. We use our same embedding model to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elNIp0_BdyXo"
   },
   "outputs": [],
   "source": [
    "#search_prompt = 'denim jeans, loose fit, high-waist'\n",
    "search_prompt = 'Oversized Sweaters'\n",
    "\n",
    "query_vector = embedding_model.embed_query(search_prompt)\n",
    "print(f'query vector length: {len(query_vector)}')\n",
    "print(f'query vector sample: {query_vector[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7yJwN1fxZACG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can take that and use it in a Cypher query with the vector index to retrieve semantically similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRTY5LnPdyXp"
   },
   "outputs": [],
   "source": [
    "result=gds.run_cypher('''\n",
    "CALL db.index.vector.queryNodes(\"product_embedding\", 10, $queryVector)\n",
    "YIELD node AS product, score\n",
    "RETURN product.id AS id,\n",
    "    product.text AS text,\n",
    "    score\n",
    "''', params={'queryVector': query_vector})\n",
    "pretty_print(result,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLmyZ_5lhTRm"
   },
   "source": [
    "### Vector Search Using Langchain\n",
    "\n",
    "We can also do this vector search with Langchain. As we'll see later, this will allow us more flexibility. To do this, we use the Neo4jVector class and call the below method to set up from an existing index in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofoi5aJvekeF"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "\n",
    "kg_vector_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=HOST,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='product_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cz1VqGy9ZACG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Langchain can handle embedding the query vector and retrieving from Neo4j behind the scenes, making our lives easier.  Langchain uses a similar query as above and retrieves the `text` property we set for each Product node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDFoMXmbekeF"
   },
   "outputs": [],
   "source": [
    "res = kg_vector_search.similarity_search_with_score(search_prompt, k=10)\n",
    "# Visualize as a dataframe\n",
    "pretty_print(pd.DataFrame([{'document': d.page_content, 'score': score} for d, score in res]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IXn_6bFCekeF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Try Yourself\n",
    "\n",
    "Experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHayKAOlekeF"
   },
   "outputs": [],
   "source": [
    "res = kg_vector_search.similarity_search_with_score('turtle neck', k=10)\n",
    "# Visualize as a dataframe\n",
    "pretty_print(pd.DataFrame([{'document': d.page_content, 'score': score} for d, score in res]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tofjZIsSekeF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Semantic Search with Context (Graph Patterns)\n",
    "__Using Graph Patterns to Improve Context in Search & Retrieval__\n",
    "\n",
    "Above, we saw how you can use the vector index to find semantic similar products in user searches.  This is an extremely powerful tool; however, it is not the end-all be-all.  It doesn't consider much of the customer data and isn't very personalized. Furthermore, some search\n",
    "prompts, like \"Oversized Sweater,\" are very general and can match a large number of products, many of which won't be relevant to the specific user conducting the search.\n",
    "\n",
    "We have a rich knowledge graph full of customer information; let's see how to leverage it to improve search experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0NrRQaY3ZACH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Further graph exploration\n",
    "\n",
    "#### Move to the Neo4j Browser\n",
    "Let's have another look at the data in the <a href=\"https://browser.neo4j.io/index.html\" target=\"_blank\">Neo4j Browser</a>!\n",
    "\n",
    "The browserguide this time is:\n",
    "\n",
    "`:play https://metis.graphdatabase.ninja/bredags/customer.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QdiPqIq2dyXp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Personalizing Results Based on Customer Behavior in the Graph\n",
    "\n",
    "As we saw in Browser, an important piece of information expressed in this graph, but not directly in the product documents and text embeddings, is customer purchasing behavior.  We saw that we can use graph patterns in Cypher to extract insights from these. Now that we know how this pattern works, we can apply it to our semantic search to make results more personalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search Using Cypher\n",
    "\n",
    "To do this, we append a MATCH statement to the end of our initial vector search query.  Basically, once the product documents are returned, we can re-calculate how they would score according to the query above and use that to re-rank the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMER_ID = \"daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0\"\n",
    "\n",
    "result = gds.run_cypher('''\n",
    "CALL db.index.vector.queryNodes(\"product_embedding\", 250, $queryVector)\n",
    "YIELD node AS product, score as searchScore\n",
    "OPTIONAL MATCH (product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {id: $CUSTOMER_ID})\n",
    "WITH count(a) AS purchaseScore, product.text AS text, searchScore, product.id AS productCode\n",
    "RETURN text,\n",
    " (1+purchaseScore)*searchScore AS score, {productCode: productCode, purchaseScore:purchaseScore, searchScore:searchScore} AS metadata\n",
    " ORDER BY purchaseScore DESC, searchScore DESC LIMIT 15\n",
    "''', params={'queryVector': query_vector, 'CUSTOMER_ID': CUSTOMER_ID})\n",
    "\n",
    "pretty_print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "L0ySzeloZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Semantic Search Using LangChain\n",
    "\n",
    "LangChain allows a `retrieval_query` argument do do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eKwXKf_ekeF"
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID = \"daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0\"\n",
    "\n",
    "kg_personalized_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=HOST,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='product_embedding',\n",
    "    retrieval_query=f\"\"\"\n",
    "    WITH node AS product, score AS searchScore\n",
    "\n",
    "    OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "    -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{id: '{CUSTOMER_ID}'}})\n",
    "\n",
    "    WITH count(a) AS purchaseScore, product.text AS text, searchScore, product.id AS productCode\n",
    "    WITH purchaseScore, (1+purchaseScore)*searchScore AS score, searchScore, text, productCode\n",
    "    RETURN text,\n",
    "        score,\n",
    "        {{productCode: productCode, purchaseScore:purchaseScore, searchScore:searchScore, score:score}} AS metadata\n",
    "    ORDER BY purchaseScore DESC, searchScore DESC LIMIT 15\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9BqGWliZZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUCSQyrZekeF"
   },
   "outputs": [],
   "source": [
    "res = kg_personalized_search.similarity_search(search_prompt, k=100)\n",
    "\n",
    "# Visualize as a dataframe\n",
    "pretty_print(pd.DataFrame([{'productCode': d.metadata['productCode'],\n",
    "               'document': d.page_content,\n",
    "               'score': d.metadata['score'],\n",
    "               'searchScore': d.metadata['searchScore'],\n",
    "               'purchaseScore': d.metadata['purchaseScore']} for d in res]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t55maEsfdyXp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Augmenting Semantic Search with Knowledge Graph Inference & ML\n",
    "\n",
    "We saw above how to use graph pattern matching to personalize semantic search and make it more contextually relevant.\n",
    "\n",
    "In addition to this, we also have [Graph Data Science algorithms and machine learning](https://neo4j.com/docs/graph-data-science/current/introduction/) which allows you to enrich your knowledge graph with additional properties, relationships, and graph metrics. These can in-turn be leveraged in search and retrieval to improve and augment results.\n",
    "\n",
    "We will walk through an example of this below, where we use Graph Data Science to augment retrieval with additional product recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vec2fJG7ZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Below, we use graph machine learning to create relationships that can help us make personalized recommendations based on purchase history.\n",
    "\n",
    "\n",
    "We do this by leveraging Node Embeddings with K-Nearest Neighbor (KNN). Specifically we:\n",
    "1. Use Node embeddings to encode the similarity between articles based on shared customer search history\n",
    "2. Take those node embeddings as input to KNN, an unsupervised learning technique, to link the most similar products together with a \"CUSTOMERS_ALSO_LIKE\" relationship.\n",
    "3. We can then use Cypher patterns at query time to grab recommended items based on a customer's recent purchase history.\n",
    "This process helps scale memory-based recommendation techniques to larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtGT6HfgekeG"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from neo4j_tools import gds_utils\n",
    "\n",
    "#clear past GDS analysis in the case of re-running\n",
    "gds_utils.clear_all_gds_graphs(gds)\n",
    "gds_utils.delete_relationships('CUSTOMERS_ALSO_LIKE', gds, src_node_label='Article')\n",
    "\n",
    "\n",
    "# graph projection - project co-purchase graph into analytics workspace\n",
    "gds.run_cypher('''\n",
    "   MATCH (a1:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(a2:Article)\n",
    "   WITH gds.graph.project(\"proj\", a1, a2,\n",
    "       {sourceNodeLabels: labels(a1),\n",
    "       targetNodeLabels: labels(a2),\n",
    "       relationshipType: \"COPURCHASE\"}) AS g\n",
    "   RETURN g.graphName\n",
    "   ''')\n",
    "g = gds.graph.get(\"proj\")\n",
    "\n",
    "# create FastRP node embeddings\n",
    "gds.fastRP.mutate(g, mutateProperty='embedding', embeddingDimension=128, randomSeed=7474, concurrency=4, iterationWeights=[0.0, 1.0, 1.0])\n",
    "\n",
    "# draw KNN\n",
    "knn_stats = gds.knn.write(g, nodeProperties=['embedding'], nodeLabels=['Article'],\n",
    "                  writeRelationshipType='CUSTOMERS_ALSO_LIKE', writeProperty='score',\n",
    "                  sampleRate=1.0, initialSampler='randomWalk', concurrency=1, similarityCutoff=0.75, randomSeed=7474)\n",
    "\n",
    "# write embeddings back to database to introspect later\n",
    "gds.graph.writeNodeProperties(g, ['embedding'], ['Article'])\n",
    "\n",
    "# clear graph projection once done\n",
    "g.drop()\n",
    "\n",
    "# output knn stats\n",
    "knn_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Zu6MWuaDekeG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize Node Embeddings\n",
    "To better help understand what the Node embeddings are doing, letâ€™s pull some back and visualize them!\n",
    "\n",
    "__NOTE__: The visualization below should be pre-rendered to cut down on runtime. Running the TSNE cell can take ~5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35pfsqVwdyXq"
   },
   "outputs": [],
   "source": [
    "graph_emb_df = gds.run_cypher('''\n",
    "MATCH (p:Product)<-[:VARIANT_OF]-(a:Article)-[:LOCATED_IN]->(d)\n",
    "RETURN a.id AS articleId,\n",
    "    p.name AS productName,\n",
    "    p.type AS productTypeName,\n",
    "    d.name AS departmentName,\n",
    "    p.description AS detailDesc,\n",
    "    a.embedding AS embedding\n",
    "''')\n",
    "\n",
    "#view a sample\n",
    "pretty_print(graph_emb_df.loc[:5, ['articleId', 'embedding']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNXMIDaGekeG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "#\n",
    "df = graph_emb_df.copy()\n",
    "filtered_node_df = df[df.embedding.apply(lambda x: np.count_nonzero(x) > 0)].reset_index(drop=True)\n",
    "# instantiate the TSNE model\n",
    "tsne = TSNE(n_components=2, random_state=7474, init='random', learning_rate=\"auto\")\n",
    "# Use the TSNE model to fit and output a 2-d representation\n",
    "E = tsne.fit_transform(np.stack(filtered_node_df['embedding'], axis=0))\n",
    "coord_df = pd.concat([filtered_node_df, pd.DataFrame(E, columns=['x', 'y'])], axis=1)\n",
    "\n",
    "pretty_print(coord_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7yNBed6eekeG"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from sklearn.manifold import TSNE\n",
    "#\n",
    "alt.data_transformers.disable_max_rows()\n",
    "chart = alt.Chart(coord_df.sample(n=5000, random_state=7474)).mark_circle(size=60).encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    tooltip=['productName', 'productTypeName', 'departmentName' ,'detailDesc']\n",
    ").properties(title=\"Article Embedding (2D Representation)\", width=750, height=700)\n",
    "chart = chart.configure_axis(titleFontSize=20)\n",
    "chart.configure_legend(labelFontSize = 20)\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7nNobdJhekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Personalized Recommendations\n",
    "\n",
    "Now, let's construct a KG store to retrieve recommendations for a user.  This need not be based on a user prompt or vector search. Instead, we will make it purely based on purchase history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVy9J6T5ekeH"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "kg = Neo4jGraph(url=HOST, username=USERNAME, password=PASSWORD, database=DATABASE)\n",
    "\n",
    "res = kg.query('''\n",
    "    MATCH(:Customer {id:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.id AS productCode,\n",
    "        product.name AS prodName,\n",
    "        product.type AS productType,\n",
    "        product.text AS document,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    ''', params={'customerId': CUSTOMER_ID, 'k':15})\n",
    "\n",
    "#visualize as dataframe. result is list of dict\n",
    "pretty_print(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "90E9HGu4dyXq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LLM For Generating Grounded Content\n",
    "\n",
    "Let's use an LLM to automatically generate content for targeted marketing campaigns grounded with our knowledge graph using the above tools.\n",
    "Here is a quick example for generating promotional messages, but you can create all sorts of content with this!\n",
    "\n",
    "For our first message, let's consider a scenario where a user recently searched for products, but perhaps didn't commit to a purchase yet. We now want to send a message to promote relevant products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JI9LVEdKekeH"
   },
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "#Instantiate LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=LLM, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8I6JesV0ekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Create Knowledge Graph Stores for Retrieval\n",
    "\n",
    "To ground our content generation, we need to define retrievers to pull information from our knowledge graph.  Let's make three stores:\n",
    "1. Personalized Search Retriever (`kg_personalized_search`): Based on recent customer searches and purchase history, pull relevant products.\n",
    "2. Recommendations retriever (`kg_recommendations_app`): Based on our Graph ML, what else can we recommend to them to pair with the relevant products?\n",
    "3. Customer information (`kg_customer_info`): Information we have on the customer, in this case that's only the name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLBBVRXwdyXq"
   },
   "outputs": [],
   "source": [
    "# This will be a function so we can change per customer id\n",
    "# We will use a mock URL for our sources in the metadata\n",
    "def kg_personalized_search_gen(customer_id):\n",
    "    return Neo4jVector.from_existing_index(\n",
    "        embedding=embedding_model,\n",
    "        url=HOST,\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "        database=DATABASE,\n",
    "        index_name='product_embedding',\n",
    "        retrieval_query=f\"\"\"\n",
    "        WITH node AS product, score AS searchScore\n",
    "\n",
    "        OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "        -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{id: '{customer_id}'}})\n",
    "        WITH count(a) AS purchaseScore, product, searchScore\n",
    "        RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.id  AS text,\n",
    "            (1.0+purchaseScore)*searchScore AS score,\n",
    "            {{source: 'https://representative-domain/product/' + product.id}} AS metadata\n",
    "        ORDER BY purchaseScore DESC, searchScore DESC LIMIT 5\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# Use the same personalized recommendations as above\n",
    "def kg_recommendations_app(customer_id, k=30):\n",
    "    res = kg.query(\"\"\"\n",
    "    MATCH(:Customer {id:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.id  AS text,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    \"\"\", params={'customerId': customer_id, 'k':k})\n",
    "\n",
    "    return \"\\n\\n\".join([d['text'] for d in res])\n",
    "\n",
    "# get the customer's name\n",
    "def kg_customer_info(customer_id):\n",
    "    res = kg.query(\"\"\"\n",
    "    MATCH(c:Customer {id:$customerId})\n",
    "    RETURN c.name as customername\n",
    "    \"\"\", params={'customerId': customer_id})\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "i7sCt8roekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Now, let's define our prompts. We will combine two:\n",
    "1. A system prompt which, in this case, tells the LLM how to generate the message\n",
    "2. A human prompt that just wraps the customer search(es)/interest(s)\n",
    "\n",
    "This will allow us to pass the customer interest(s) to the retriever but then also to the LLM for additional context when drafting the message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUAROR6aekeI"
   },
   "outputs": [],
   "source": [
    "general_system_template = '''\n",
    "You are a personal assistant named Sam for a fashion, home, and beauty company called HRM.\n",
    "write an email to {customerName}, one of your customers, to promote and summarize products relevant for them given the current season / time of year: {timeOfYear}.\n",
    "Please only mention the products listed below. Do not come up with or add any new products to the list.\n",
    "Each product comes with an https `url` field. Make sure to provide that https url with descriptive name text in markdown for each product.\n",
    "\n",
    "---\n",
    "# Relevant Products:\n",
    "{searchProds}\n",
    "\n",
    "# Customer May Also Be Interested In the following\n",
    " (pick items from here that pair with the above products well for the current season / time of year: {timeOfYear}.\n",
    " prioritize those higher in the list if possible):\n",
    "{recProds}\n",
    "---\n",
    "'''\n",
    "general_user_template = \"{searchPrompt}\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "    HumanMessagePromptTemplate.from_template(general_user_template),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TgFbaUt6ekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Create a Chain\n",
    "\n",
    "Now let's put a chain together that will leverage the retrievers, prompts, and LLM model. This is where Langchain shines, putting RAG together in a simple way.\n",
    "\n",
    "In addition to the personalized search and recommendations context, we will allow for the `timeOfYear` so the LLM can tailor the language accordingly.\n",
    "You can potentially add other creative parameters here to help the LLM write relevant messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUpih07QdyXr"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# helper function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LLM chain\n",
    "def chain_gen(customer_id):\n",
    "    return ({'searchProds': (lambda x:x['searchPrompt']) | kg_personalized_search_gen(customer_id).as_retriever(search_kwargs={\"k\": 100}) | format_docs,\n",
    "             'recProds': (lambda x:customer_id) |  RunnableLambda(kg_recommendations_app),\n",
    "             'customerName': (lambda x:customer_id) |  RunnableLambda(kg_customer_info),\n",
    "             'timeOfYear': lambda x:x['timeOfYear'],\n",
    "             \"searchPrompt\":  lambda x:x['searchPrompt']}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JjbUGH6WekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Example Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkBdqOVjekeI"
   },
   "outputs": [],
   "source": [
    "chain = chain_gen(CUSTOMER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL6P3IoydyXr"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'searchPrompt':search_prompt,'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy3fKoC1E0CC"
   },
   "source": [
    "#### Inspecting the Prompt Sent to the LLM\n",
    "In the above run, the LLM should only be using results from our Neo4j database to populate recommendations. Run the below cell to see the final prompt that was sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7-yDDUaD6FD"
   },
   "outputs": [],
   "source": [
    "def format_final_prompt(x):\n",
    "   return f'''=== Prompt to send to LLM ===\n",
    "   {x.to_string()}\n",
    "   === End Prompt ===\n",
    "   '''\n",
    "\n",
    "def chain_print_prompt(customer_id):\n",
    "    return ({'searchProds': (lambda x:x['searchPrompt']) | kg_personalized_search_gen(customer_id).as_retriever(search_kwargs={\"k\": 100}) | format_docs,\n",
    "             'recProds': (lambda x:customer_id) |  RunnableLambda(kg_recommendations_app),\n",
    "             'customerName': (lambda x:customer_id) |  RunnableLambda(kg_customer_info),\n",
    "             'timeOfYear': lambda x:x['timeOfYear'],\n",
    "             \"searchPrompt\":  lambda x:x['searchPrompt']}\n",
    "            | prompt\n",
    "            | format_final_prompt\n",
    "            | StrOutputParser())\n",
    "\n",
    "print( chain_print_prompt(CUSTOMER_ID)\\\n",
    "      .invoke({'searchPrompt':search_prompt,'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8G_vdFviekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Feel free to experiment and try more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeOts3Q4ZACL"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'searchPrompt':\"western boots\", 'timeOfYear':'May, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1IU_gedrekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Demo App\n",
    "Now letâ€™s use the above tools to create a demo app with Gradio.  We will need to make a couple more functions, but otherwise easy to fire up from a Notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1F0ve3cekeI"
   },
   "outputs": [],
   "source": [
    "# Create a means to generate and cache chains...so we can quickly try different customer ids\n",
    "personalized_search_chain_cache = dict()\n",
    "def get_chain(customer_id):\n",
    "    if customer_id in personalized_search_chain_cache:\n",
    "        return personalized_search_chain_cache[customer_id]\n",
    "    chain = chain_gen(customer_id)\n",
    "    personalized_search_chain_cache[customer_id] = chain\n",
    "    return chain\n",
    "\n",
    "# create multiple demo examples to try\n",
    "examples = [\n",
    "    [\n",
    "        CUSTOMER_ID,\n",
    "        'Feb, 2024',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'May, 2024',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '44b0898ecce6cc1268dfdb0f91e053db014b973f67e34ed8ae28211410910693',\n",
    "        'Nov, 2024',\n",
    "        'Cowboy boots'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'Feb, 2024',\n",
    "        'denim jeans'\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsBcFQLlekeI"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def message_generator(*x):\n",
    "    chain = get_chain(x[0])\n",
    "    return chain.invoke({'searchPrompt':x[2], 'timeOfYear': x[1]})\n",
    "\n",
    "customer_id = gr.Textbox(value=CUSTOMER_ID, label=\"Customer ID\")\n",
    "time_of_year = gr.Textbox(value=\"Feb, 2024\", label=\"Time Of Year\")\n",
    "search_prompt_txt = gr.Textbox(value='Oversized Sweaters', label=\"Customer Interests(s)\")\n",
    "message_result = gr.Markdown( label=\"Message\")\n",
    "\n",
    "demo = gr.Interface(fn=message_generator,\n",
    "                    inputs=[customer_id, time_of_year, search_prompt_txt],\n",
    "                    outputs=message_result,\n",
    "                    examples=examples,\n",
    "                    title=\"ðŸª„ Message Generator ðŸ¥³\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Tucjvc73ZACM",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## That's a Wrap!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
